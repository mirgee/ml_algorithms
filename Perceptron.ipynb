{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Give biological introduction, provide two images - one biological, one artificial\n",
    "* Explain how the algorithm works mathematically\n",
    "* Maybe give an animation of how the discriminative line moves\n",
    "* They can be trained to implement an AND gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the weights\n",
    "2. For each training sample $\\mathbf{x}^{(i)}$\n",
    "    3. Compute $\\hat{y}^{(i)} = \\phi (\\mathbf{w}^T \\mathbf{x}^{(i)})$\n",
    "    4. Compute $\\mathbf{w} := \\mathbf{w} + \\eta(y^{(i)} - \\hat{y}^{(i)}) \\mathbf{x}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the case that $y^{(i)} = \\hat{y}^{(i)}$ the weights remain unchanged\n",
    "* In the case that $y^{(i)} \\neq \\hat{y}^{(i)}$ the weights are moved in the direction of $\\mathbf{x}^{(i)}$. This is because if $\\hat{y}^{(i)}$ is negative and it should be positive, i.e. $z$ is too small, we  $2\\eta \\mathbf{x}^{(i)}$, i.e. the vectors w and x are too orthogonal, so we should move in the direciton of the other (but we can't move x). Adding multiple of the vector ensures that. The other case is analogous, but instead we subtract it  (move it away).\n",
    "* The classes must be linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, num_iters=100, learning_rate=1, epsilon=0.01, init_mean=0.0, init_std=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.w_init = w_init\n",
    "        self.random_state = 0\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.loss_history_ = []\n",
    "        \n",
    "        self._init_weights(X)\n",
    "        \n",
    "        for _ in range(self.num_iters):\n",
    "            num_errors = 0\n",
    "            for x, y in zip(X, y):\n",
    "                y_hat = self._predict(x, self.w_)\n",
    "                num_errors += 1 if y_hat != y else 0\n",
    "                self_update_weights(y_hat, y)\n",
    "            loss = num_errors/len(X)\n",
    "            self.loss_history_.append(loss)\n",
    "            if loss < self.epsilon:\n",
    "                break\n",
    "                \n",
    "    def _step_function(self, z):\n",
    "        return 1 if z >= 0 else -1\n",
    "    \n",
    "    def _update_weights(self, y_hat, y):\n",
    "        update = self.learning_rate*(y_hat-y)\n",
    "        self.w_[1:] += update*x\n",
    "        self.w_[0] += update\n",
    "    \n",
    "    def _predict(self, x, w):\n",
    "        return np.dot(x, w[1:])\n",
    "    \n",
    "    def _init_weights(self, X):\n",
    "        if self.w_init == 'zeros':\n",
    "            self.w_ = np.zeros(X.shape[1]+1)\n",
    "        elif self.w_init == 'normal':\n",
    "            rgen = np.random.RandomState(self.random_state)\n",
    "            self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]+1)\n",
    "        else:\n",
    "            raise ValueError(\"w_init must be one of ('zeros', 'normal')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
